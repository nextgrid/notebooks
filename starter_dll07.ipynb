{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z_3rqYh5nGlR"
   },
   "source": [
    "# Preparation part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCro1x2fnF1P"
   },
   "source": [
    "Import basic dependencies needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UK1QceomgjC"
   },
   "outputs": [],
   "source": [
    "# Create virtual display to render on remote machine\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1, 1))\n",
    "display.start()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAXmMFw_sBiI"
   },
   "source": [
    "Function for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gm-rLvu9nMiE"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, env, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a RL agent\n",
    "    :param model: (BaseRLModel object) the RL Agent\n",
    "    :param num_episodes: (int) number of episodes\n",
    "    :return: (list) List of rewards for episodes\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    obs = env.reset()\n",
    "    for i in range(num_episodes):\n",
    "        episode_rewards.append(0.0)\n",
    "        done = False\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            # Stats\n",
    "            episode_rewards[-1] += reward\n",
    "            if done:\n",
    "                obs = env.reset()\n",
    "    # Compute mean reward for the last 100 episodes\n",
    "    mean_100ep_reward = round(np.mean(episode_rewards), 1)\n",
    "    print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
    "    return episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiR1EpFIs382"
   },
   "source": [
    "# Training part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rDJn9CWhnEXh"
   },
   "source": [
    "## Basic cartpole solution\n",
    "In this example we will use PPO2 (be aware of different hyperparameters when changing the algorithm):  \n",
    "**Full list** of algos available with stable-baselines: https://stable-baselines.readthedocs.io/en/master/guide/algos.html  \n",
    "**PPO2 documentation:** https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "f_dn7Yu8m9x2",
    "outputId": "ce6800ce-beb6-4bde-fa40-991bf7e5f045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy # our policy (neural network that will perform actions)\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2 # import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard\n",
    "We also can add tensorboard when we train stable-baselines, then we will be able to explore more details about out training process.\n",
    "To use tensorboard you need:\n",
    "1. Pass tensorboard_log argument to you model which will indicate path where you want to have logs written;\n",
    "2. Run tensorboard separately (preferred) or inside the notebook to explore the logs. To run new tensorboard instance you need to execute in terminal:   \n",
    "    - `tensorboard --logdir <path to logs>\"` then we will see port of the tensorboard and can open it via browser just replacing the port of jupyter (8888) with the port of the tensorboard (default is 6006);\n",
    "\n",
    "Here we set tensorboard logs directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir = \"/root/tensorboard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_7fKePMtCe7"
   },
   "source": [
    "Create an instance with CartPole-v0 environment for our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "UBPzaTbEnlQq",
    "outputId": "79cd1b63-954d-4e5a-ad62-dbe670f4704d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = DummyVecEnv([lambda: env]) # Should be used in 1 core case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9EpDF-ntg81"
   },
   "source": [
    "Train our model. Note that we use **magic command ```%%time```** to track time for this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PQvrSiYNn3Vb",
    "outputId": "03bb8e0e-aa44-48f2-a448-f9579a6aa204"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/common/tf_util.py:58: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/common/tf_util.py:67: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/common/policies.py:115: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/common/policies.py:560: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/layers/core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/a2c/utils.py:156: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/common/distributions.py:326: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/common/distributions.py:327: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/ppo2/ppo2.py:194: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/ppo2/ppo2.py:202: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/ppo2/ppo2.py:210: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/ppo2/ppo2.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/ppo2/ppo2.py:246: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/common/base_class.py:1082: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/stable_baselines/a2c/utils.py:581: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "-------------------------------------\n",
      "| approxkl           | 0.007997252  |\n",
      "| clipfrac           | 0.119140625  |\n",
      "| explained_variance | 0.00387      |\n",
      "| fps                | 269          |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 0.6859102    |\n",
      "| policy_loss        | -0.017660078 |\n",
      "| serial_timesteps   | 128          |\n",
      "| time_elapsed       | 3.58e-06     |\n",
      "| total_timesteps    | 128          |\n",
      "| value_loss         | 56.71531     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0013344522  |\n",
      "| clipfrac           | 0.00390625    |\n",
      "| explained_variance | 0.11          |\n",
      "| fps                | 774           |\n",
      "| n_updates          | 100           |\n",
      "| policy_entropy     | 0.5843034     |\n",
      "| policy_loss        | -0.0017915424 |\n",
      "| serial_timesteps   | 12800         |\n",
      "| time_elapsed       | 14.8          |\n",
      "| total_timesteps    | 12800         |\n",
      "| value_loss         | 114.06693     |\n",
      "--------------------------------------\n",
      "CPU times: user 53.3 s, sys: 11.6 s, total: 1min 4s\n",
      "Wall time: 30.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x7fafa92afbe0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "n_steps = 128\n",
    "model = PPO2(\n",
    "    MlpPolicy, \n",
    "    env, \n",
    "    verbose=1, \n",
    "    n_steps=n_steps, \n",
    "    learning_rate=0.003, \n",
    "    ent_coef=0.0,\n",
    "    tensorboard_log=tensorboard_dir\n",
    ")\n",
    "model.learn(total_timesteps=25000, log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8oP6lHVRuHSo"
   },
   "source": [
    "And **evaluation** of trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "KmzINbngoFS_",
    "outputId": "4adefb4a-0f76-407a-abc6-3177788e18ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 200.0 Num episodes: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = evaluate_model(model, env, num_episodes=100)\n",
    "np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rCMP8vsnuVS6"
   },
   "source": [
    "This task considired to be solved if the model achieved reward higher than 190 and maximum reward is 200.  \n",
    "You should see that it took us around 30 seconds to train PPO2 on cartpole in colab.  \n",
    "However, we can make it faster if we will parallelize our environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hi67p34qx_sO"
   },
   "source": [
    "## Parallelized environment on cartpole example\n",
    "Here we can simply use baselines documentation as a cookbook and run our traing in parallel.  \n",
    "https://stable-baselines.readthedocs.io/en/master/guide/examples.html#multiprocessing-unleashing-the-power-of-vectorized-environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZsINQYPpidx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines.common import set_global_seeds, make_vec_env\n",
    "from stable_baselines import PPO2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iANjyXFsKC4b"
   },
   "source": [
    "Here we use make_vec_env that will create copies of environment for each CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "lireuargyluA",
    "outputId": "7311847f-5177-447d-9fbd-35bc855c03e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "num_cpu = 8\n",
    "vec_env = make_vec_env(env_name, n_envs=num_cpu, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WOfSsTbJ0K4"
   },
   "source": [
    "You need to scale you number of steps (```n_steps```) to the CPU number to have the same results, because parameter defines number of steps per one core, which are later aggregated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "sOWUgAFyzIYU",
    "outputId": "2176f802-c3fe-4f77-d772-8d3b01e08caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| approxkl           | 0.0002593151  |\n",
      "| clipfrac           | 0.0           |\n",
      "| ep_len_mean        | 13.5          |\n",
      "| ep_reward_mean     | 13.5          |\n",
      "| explained_variance | -0.00439      |\n",
      "| fps                | 347           |\n",
      "| n_updates          | 1             |\n",
      "| policy_entropy     | 0.6928569     |\n",
      "| policy_loss        | 0.00090540387 |\n",
      "| serial_timesteps   | 16            |\n",
      "| time_elapsed       | 4.05e-06      |\n",
      "| total_timesteps    | 128           |\n",
      "| value_loss         | 22.222218     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| approxkl           | 0.009314638   |\n",
      "| clipfrac           | 0.12890625    |\n",
      "| ep_len_mean        | 109           |\n",
      "| ep_reward_mean     | 109           |\n",
      "| explained_variance | 0.194         |\n",
      "| fps                | 2052          |\n",
      "| n_updates          | 100           |\n",
      "| policy_entropy     | 0.38300434    |\n",
      "| policy_loss        | -0.0047525167 |\n",
      "| serial_timesteps   | 1600          |\n",
      "| time_elapsed       | 6.7           |\n",
      "| total_timesteps    | 12800         |\n",
      "| value_loss         | 215.05116     |\n",
      "--------------------------------------\n",
      "CPU times: user 28.9 s, sys: 8.86 s, total: 37.7 s\n",
      "Wall time: 13.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x7fb027583668>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = PPO2(\n",
    "    MlpPolicy, \n",
    "    vec_env, \n",
    "    verbose=1, \n",
    "    n_steps=n_steps // num_cpu, # Scaling number of steps\n",
    "    learning_rate=0.003, \n",
    "    ent_coef=0.0,\n",
    "    tensorboard_log=tensorboard_dir\n",
    ")\n",
    "model.learn(total_timesteps=25000, log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OT-wV6FcKWMR"
   },
   "source": [
    "### Saving the model\n",
    "To evaluate our model on the same function we need to have it one CPU environment. However, stable-baselines doesn't provide easy way to transfer our model, so we will simply **save our model** in the storage and then load it with needed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e34Bb3bSGsX9"
   },
   "outputs": [],
   "source": [
    "model.save(\"ppo2_parallel_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X71BX8LfBzDK"
   },
   "source": [
    "Loading the model with new environment (for on CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "JN9j_v1mG953",
    "outputId": "6ba800ea-1435-4856-ea3d-6353ac3467b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO2.load(\"ppo2_parallel_test\", env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1O34Z93_K1V0"
   },
   "source": [
    "And model **evaluation** again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "NBBpK27dzPTl",
    "outputId": "2bd34ea6-fa54-4e92-b1ee-3af33631ac60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 200.0 Num episodes: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = evaluate_model(model, env, num_episodes=100)\n",
    "np.std(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1plVwIMvK7lw"
   },
   "source": [
    "As you can see using 4 CPUs we didn't get 4 times speed-up, but it still significantly faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tr9OH4yTLGjD"
   },
   "source": [
    "# Bonus: Changing architecture of our policy network\n",
    "Policy is neural network that performs actions and it's possible to configure it architecture with stable-baselines tools.  \n",
    "https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Moka4-nFrTl"
   },
   "source": [
    "Here we will use baselines FeedForwardPolicy network to construct our custom feed forward net.  \n",
    "We define `CustomPolicy` that will consist of two networks:\n",
    "- Policy with 2 layers of 64 neurons each;\n",
    "- Value function with 3 layers: 1 and 2 have 64 neurons and the last has 32;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcxflyFiziqu"
   },
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import FeedForwardPolicy, register_policy\n",
    "\n",
    "# Custom MLP policy of three layers of size 128 each\n",
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(*args, **kwargs,\n",
    "                                           net_arch=[dict(pi=[64, 64],          # Policy network layers size\n",
    "                                                          vf=[64, 64, 32])],    # Value function layers size\n",
    "                                           feature_extraction=\"mlp\") # The feature extraction type (\"cnn\" or \"mlp\") (could be also custom network)\n",
    "\n",
    "# Register the policy, it will check that the name is not already taken\n",
    "register_policy('CustomPolicy', CustomPolicy)\n",
    "\n",
    "# Because the policy is now registered, you can pass\n",
    "# a string to the agent constructor instead of passing a class\n",
    "# model = A2C(policy='CustomPolicy', env='LunarLander-v2', verbose=1).learn(total_timesteps=100000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DF1sbBU0E9rL"
   },
   "source": [
    "Creating our vectorized (multi-CPU) environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "gU_8bLZiA8fG",
    "outputId": "42f78d35-e85e-4195-fdf5-0c4aa8a49c99"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v0\"\n",
    "num_cpu = 4\n",
    "vec_env = make_vec_env(env_name, n_envs=num_cpu, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YDLN4t-jE-jm"
   },
   "source": [
    "The same steps as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "AnOyi0pEQoEv",
    "outputId": "96829fa9-4202-477a-d52f-8a681197a116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| approxkl           | 0.0070039397 |\n",
      "| clipfrac           | 0.107421875  |\n",
      "| ep_len_mean        | 19.4         |\n",
      "| ep_reward_mean     | 19.4         |\n",
      "| explained_variance | 0.0272       |\n",
      "| fps                | 300          |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 0.68663955   |\n",
      "| policy_loss        | -0.011830209 |\n",
      "| serial_timesteps   | 32           |\n",
      "| time_elapsed       | 3.1e-06      |\n",
      "| total_timesteps    | 128          |\n",
      "| value_loss         | 27.326387    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0051283794 |\n",
      "| clipfrac           | 0.07421875   |\n",
      "| ep_len_mean        | 123          |\n",
      "| ep_reward_mean     | 123          |\n",
      "| explained_variance | 0.877        |\n",
      "| fps                | 1435         |\n",
      "| n_updates          | 100          |\n",
      "| policy_entropy     | 0.4683259    |\n",
      "| policy_loss        | 0.0047310414 |\n",
      "| serial_timesteps   | 3200         |\n",
      "| time_elapsed       | 9.26         |\n",
      "| total_timesteps    | 12800        |\n",
      "| value_loss         | 6.9264555    |\n",
      "-------------------------------------\n",
      "CPU times: user 26.3 s, sys: 1.88 s, total: 28.2 s\n",
      "Wall time: 20.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines.ppo2.ppo2.PPO2 at 0x7f7d8e74d978>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = PPO2(\n",
    "    CustomPolicy, # Instead of MlpPolicy now we have our custom policy\n",
    "    vec_env, \n",
    "    verbose=1, \n",
    "    n_steps=n_steps // num_cpu, # Scaling number of steps\n",
    "    learning_rate=0.003, \n",
    "    ent_coef=0.0,\n",
    "    tensorboard_log=tensorboard_dir\n",
    ")\n",
    "model.learn(total_timesteps=25000, log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wKa6if3E_VT"
   },
   "source": [
    "Save and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMit5RAhBXNx"
   },
   "outputs": [],
   "source": [
    "model.save(\"custom_ppo2_parallel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "LjfBu9J-BXWq",
    "outputId": "cb499c6b-6684-4b90-f1d0-d4d8769a029e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO2.load(\"custom_ppo2_parallel\", env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UaSacQ-FAPr"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Wvn-DwzeCtTu",
    "outputId": "7c2b3edc-a411-4df0-9d1b-a2810b9b8a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 200.0 Num episodes: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards = evaluate_model(model, env, num_episodes=100)\n",
    "np.std(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfkw9VfdC2hq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dll07.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
